[
    {
        "task_instruction": "The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.",
        "task_name": "task020",
        "examples": "[input]=\"Sentence: Jack played basketball for an hour after school, after which he was very tired. \nQuestion: How long did Jack play basketball?\"\n[output]=\"Yes.\"\n\n[input]=\"Sentence: He was born in China, so he went to the Embassy at 1 pm to apply for a U.S. Visa. \nQuestion: When did he go to Embassy?\"\n[output]=\"Yes.\"\n\n",
        "expected_content": "\"Sentence\" and \"Question\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["No.", "Yes."]
    },
    {
        "task_instruction": "In this task, you need to write a topic word from the given fact. The topic word must have at least one word overlap with the given fact. The topic word often involves adding a new word from a related concept. In your topic word, use at least one word from the given fact. Topic words with two or more words work best.",
        "task_name": "task036",
        "examples": "[input]=\"Fact: pesticides cause pollution.\"\n[output]=\"pollution harms.\"\n\n[input]=\"Fact: pesticides cause pollution.\"\n[output]=\"modern farming pesticide.\"\n\n[input]=\"Fact: a solar panel converts sunlight into electricity.\"\n[output]=\"sunlight sun.\"\n\n[input]=\"Fact: running requires a lot of energy.\"\n[output]=\"marathon running.\"\n\n[input]=\"Fact: soil is formed by rocks eroding.\"\n[output]=\"rain erode rock.\"\n\n[input]=\"Fact: a plant requires water for survival.\"\n[output]=\"rain water.\"\n\n[input]=\"Fact: a radio converts electrical energy into vibrations.\"\n[output]=\"vibrations sound.\"\n\n[input]=\"Fact: habitat destruction causes animals to move to find shelter in another habitat.\"\n[output]=\"chopping down trees habitat destruction.\"\n\n[input]=\"Fact: a protractor is used for measuring the angles of a triangular object.\"\n[output]=\"prism triangular.\"\n\n[input]=\"Fact: lightning can cause a forest fire.\"\n[output]=\"forest fire harms.\"\n\n",
        "expected_content": "Fact",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ]
    },
    {
        "task_instruction": "Generate an overlapping word between the given two sentences. When you find the overlapping words, they don't have to match exactly, e.g., \"survival\" and \"survive\" are valid overlapping words. Little words like \"the\" or \"of\" don't count! You must generate significant words which are not the stop words.",
        "task_name": "task039",
        "examples": "[input]=\"Sentence1: pesticides cause pollution. \nSentence2: pollution can harm animals.\"\n[output]=\"pollution.\"\n\n[input]=\"Sentence1: a solar panel converts sunlight into electricity. \nSentence2: sunlight comes from the sun.\"\n[output]=\"sunlight.\"\n\n[input]=\"Sentence1: running requires a lot of energy. \nSentence2: doing a marathon requires a lot of energy.\"\n[output]=\"requires.\"\n\n[input]=\"Sentence1: soil is formed by rocks eroding. \nSentence2: rain can help form soil.\"\n[output]=\"form.\"\n\n[input]=\"Sentence1: a plant requires water for survival. \nSentence2: rain helps plants to survive.\"\n[output]=\"survive.\"\n\n[input]=\"Sentence1: vibrations in the air form sound. \nSentence2: a radio converts electrical energy into sound.\"\n[output]=\"sound.\"\n\n[input]=\"Sentence1: chopping down trees results in habitat destruction. \nSentence2: chopping down trees causes animals to move to another habitat.\"\n[output]=\"chopping.\"\n\n[input]=\"Sentence1: a protractor is used for measuring the angles of a triangular object. \nSentence2: a prism has a triangular shape.\"\n[output]=\"triangular.\"\n\n[input]=\"Sentence1: lightning can cause a forest fire. \nSentence2: lightning can cause harm to animals.\"\n[output]=\"lightning.\"\n\n",
        "expected_content": "Sentence1 and Sentence2",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you're given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the letters E, C, and N respectively.",
        "task_name": "task190",
        "examples": "[input]=\"Sentence 1: Jon saw his friend Tom coming out of the grocery store with a bag of fruit. Sentence 2: Tom had been shopping for fruit to give Jon.\"\n[output]=\"N\"\n\n[input]=\"Sentence 1: The girl transferred all the flowers from the boquet to a vase. Sentence 2: The flowers will soon wither.\"\n[output]=\"N\"\n\n[input]=\"Sentence 1: The skier was on the edge of the ramp. Sentence 2: The skier was dressed in winter clothes.\"\n[output]=\"E\"\n\n[input]=\"Sentence 1: Joyce likes to eat fruit salad as often as possible. Sentence 2: Joyce loves eating healthy.\"\n[output]=\"E\"\n\n[input]=\"Sentence 1: The boy skated down the staircase railing. Sentence 2: The boy is a newbie skater.\"\n[output]=\"C\"\n\n[input]=\"Sentence 1: Bertha was selected as captain of her basketball team. Sentence 2: Bertha was not atheletically inclined.\"\n[output]=\"C\"\n\n",
        "expected_content": "\"Sentence 1\" and \"Sentence 2\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match"
    },
    {
        "task_instruction": "In this task, you're given a pair of sentences, sentence 1 and sentence 2. Your job is to determine if the two sentences clearly agree/disagree with each other, or if this can't be determined. Indicate your answer as yes or no respectively.",
        "task_name": "task199",
        "examples": "[input]=\"Sentence 1: Next to the MGM Grand you will find M and M World. Sentence 2: The candy has many fans who love its attractions.\"\n[output]=\"no\"\n\n[input]=\"Sentence 1: I've forgotten his name now, confessed Tuppence. Sentence 2: Tuppence remembered his name later.\"\n[output]=\"no\"\n\n[input]=\"Sentence 1: One of the first organizational realignments taking place is in the Office of the Taxpayer Advocate. Sentence 2: The office of the taxpayer advocate is having an organizational realignment.\"\n[output]=\"yes\"\n\n[input]=\"Sentence 1: yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range. Sentence 2: The tennis shoes have only one price.\"\n[output]=\"yes\"\n\n",
        "expected_content": "\"Sentence 1\" and \"Sentence 2\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["yes", "no"]
    },
    {
        "task_instruction": "In this task, you're given a statement and three sentences as choices. Your job is to determine which sentence can be inferred from the statement. Incorrect choices change the meaning in important ways or have details that are not mentioned in the statement. Indicate your answer as 1,2, or 3 corresponding to the choice number of the selected sentence.",
        "task_name": "task200",
        "examples": "[input]=\"Statement: Next to the MGM Grand you will find M and M World, four stories of merchandise and memorabilia dedicated to the candy that doesn't melt in your hand. Choices: 1. The candy has many fans who love its attractions. 2. There's four stories of memorabilia dedicated to a candy. 3. That particular candy melts and becomes difficult to eat.\"\n[output]=\"2\"\n\n[input]=\"Statment: I've forgotten his name now, confessed Tuppence. Choices: 1. Tuppence forgot his name. 2.Tuppence remembered his name later. 3. Tuppence never could forget his name.\"\n[output]=\"1\"\n\n[input]=\"Statement: One of the first organizational realignments taking place is in the Office of the Taxpayer Advocate. Choices: 1. The office of the taxpayer advocate is the last to be realigned. 2. The realignment is taking place over a few weeks. 3. The office of the taxpayer advocate is having an organizational realignment.\"\n[output]=\"3\"\n\n[input]=\"Statement: yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range. Choices: 1. The tennis shoes have a range of prices. 2. The tennis shoes can be in the hundred dollar range. 3. The tennis shoes are not over hundred dollars.\"\n[output]=\"2\"\n\n",
        "expected_content": "a \"Statement\" and three \"Choices\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["1", "2", "3"]
    },
    {
        "task_instruction": "In this task, you're given a statement and three sentences as choices. Your job is to determine the neutral choice based on your inference from the statement and your commonsense knowledge. The neutral choice is a sentence that neither agrees nor disagrees with the statement. Indicate your answer as '1', '2', or '3', corresponding to the choice number of the selected sentence. If sentence X agrees with sentence Y, one's correctness follows from the other one. If sentence X disagrees with sentence Y, they can not be correct at the same time.",
        "task_name": "task201",
        "examples": "[input]=\"Statement: Next to the MGM Grand you will find M and M World, four stories of merchandise and memorabilia dedicated to the candy that doesn't melt in your hand. Choices: 1. The candy has many fans who love its attractions. 2. There's four stories of memorabilia dedicated to a candy. 3. That particular candy melts and becomes difficult to eat.\"\n[output]=\"1\"\n\n[input]=\"Statment: I've forgotten his name now, confessed Tuppence. Choices: 1. Tuppence forgot his name. 2.Tuppence remembered his name later. 3. Tuppence never could forget his name.\"\n[output]=\"2\"\n\n[input]=\"Statement: One of the first organizational realignments taking place is in the Office of the Taxpayer Advocate. Choices: 1. The office of the taxpayer advocate is the last to be realigned. 2. The realignment is taking place over a few weeks. 3. The office of the taxpayer advocate is having an organizational realignment.\"\n[output]=\"2\"\n\n[input]=\"Statement: yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range. Choices: 1. The tennis shoes have a range of prices. 2. The tennis shoes can be in the hundred dollar range. 3. The tennis shoes are not over hundred dollars.\"\n[output]=\"1\"\n\n",
        "expected_content": "a \"Statement\" and three \"Choices\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["1", "2", "3"]
    },
    {
        "task_instruction": "In this task, you're given a statement, and three sentences as choices. Your job is to determine which sentence clearly disagrees with the statement. Indicate your answer as '1', '2', or '3' corresponding to the choice number of the selected sentence.",
        "task_name": "task202",
        "examples": "[input]=\"Statement: Next to the MGM Grand you will find M and M World, four stories of merchandise and memorabilia dedicated to the candy that doesn't melt in your hand. Choices: 1. The candy has many fans who love its attractions. 2. There's four stories of memorabilia dedicated to a candy. 3. That particular candy melts and becomes difficult to eat.\"\n[output]=\"3\"\n\n[input]=\"Statment: I've forgotten his name now, confessed Tuppence. Choices: 1. Tuppence forgot his name. 2.Tuppence remembered his name later. 3. Tuppence never could forget his name.\"\n[output]=\"3\"\n\n[input]=\"Statement: One of the first organizational realignments taking place is in the Office of the Taxpayer Advocate. Choices: 1. The office of the taxpayer advocate is the last to be realigned. 2. The realignment is taking place over a few weeks. 3. The office of the taxpayer advocate is having an organizational realignment.\"\n[output]=\"1\"\n\n[input]=\"Statement: yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range. Choices: 1. The tennis shoes have a range of prices. 2. The tennis shoes can be in the hundred dollar range. 3. The tennis shoes are not over hundred dollars.\"\n[output]=\"3\"\n\n",
        "expected_content": "a \"Statement\" and three \"Choices\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["1", "2", "3"]
    },
    {
        "task_instruction": "You will be given three sentences. Read them, then identify a noun phrase (person, place, or thing) or event that is shared between all three sentences. As the output, write the span of the text corresponding to that phrase in each sentence. Keep the order of the sentences, that is, your answer should look like: 1: *a phras from sentence 1e* 2: *a phras from sentence 2* 3: *a phrase from sentence 3*",
        "task_name": "task281",
        "examples": "[input]=\"1: Four employees of the store have been arrested , but its manager -- herself a woman -- was still at large Saturday , said Goa police superintendent Kartik Kashyap . 2: If convicted , they could spend up to three years in jail , Kashyap said . 3: The four store workers arrested could spend 3 years each in prison if convicted .\"\n[output]=\"1: Four employees of the store 2: they 3: The four store workers\"\n\n[input]=\"1: Stewart said that she and her husband , Joseph Naaman , booked Felix on their Etihad Airways flight from the United Arab Emirates to New York 's John F . Kennedy International Airport on April 1 . 2: The couple said they spent $ 1,200 to ship Felix on the 14-hour flight . 3: Couple spends $ 1,200 to ship their cat , Felix , on a flight from the United Arab Emirates .\"\n[output]=\"1: their Etihad Airways flight from the United Arab Emirates to New York 's John F . Kennedy International Airport 2: the 14-hour flight 3: a flight from the United Arab Emirates\"\n\n[input]=\"1: But an Arizona official told CNN Bates never trained with the agency . 2:  He did n't come to Arizona ,  the official from the Maricopa County Sheriff 's Office said ,  and he certainly did n't train with us .  3: Maricopa County Sheriff 's Office in Arizona says Robert Bates never trained with them .\"\n[output]=\"1: never trained 2: did n't train 3: never trained\"\n\n",
        "expected_content": "three sentences with a noun phrase (person, place, or thing) or event that is shared between all three sentences",
        "optional_list": [
            "input",
        "output",
        "\n\n"
    ],
    "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you are given a statement spoken by a politician in natural language. Your task is to generate the subject of the discussion for the given statement. The subject generated is not necessarily a part of the given input. Your answer should contain one or more words.",
        "task_name": "task613",
        "examples": "[input]=\"Republicans enacted the most drastic cuts to K-12 public schools of any state in the nation.\"\n[output]=\"education\"\n\n[input]=\"Just look at what the FBI director said about her (Hillary Clinton) her misconduct is a disgrace and embarrassment to our country.\"\n[output]=\"candidates-biography\"\n\n[input]=\"When his grandfather arrived in the United States, there were no government benefits for immigrants.\"\n[output]=\"welfare\"\n\n",
        "expected_content": "a statement spoken by a politician in natural language",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ]
    },
    {
        "task_instruction": "In this task you will be given a claim and a perspective. You should determine whether that perspective supports or undermines the claim. If the perspective could possibly convince someone with different view, it is supporting, otherwise it is undermining.",
        "task_name": "task738",
        "examples": "[input]=\"claim: Music containing lyrics that glorify violent and criminal lifestyles should be banned.\n perspective: hip hop artists have a right to free speech\"\n[output]=\"undermine\"\n\n[input]=\"claim: Abolish the US Electoral College.\n perspective: The electoral college weakens incentives for voting and party building.\"\n[output]=\"support\"\n\n",
        "expected_content": "\"claim\" and \"perspective\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_",
            "therefore",
            "Therefore"
        ],
        "metric": "exact_match",
        "labels": ["support", "undermine"]
    },
    {
        "task_instruction": "In this task, you are given a premise, a hypothesis, and an update. The premise sentence describes a real-world situation and is always assumed to be true. The hypothesis sentence describes an assumption or inference that you might make about that situation having read the premise. The update provides additional information about the situation that might weaken or strengthen the hypothesis. A weakener is a statement that weakens the hypothesis. It makes you much less likely to believe the hypothesis is true. A strengthener is a statement that strengthens the hypothesis. It makes you much more likely to believe the hypothesis is true. Your task is to output 'strengthener' or 'weakener' if the update strengths or weakens the hypothesis, respectively.",
        "task_name": "task935",
        "examples": "[input]=\"Premise: PersonX seems interested\nHypothesis: PersonX then good activity\nUpdate: PersonX was a good student\"\n[output]=\"strengthener\"\n\n[input]=\"Premise: PersonX seems interested\nHypothesis: PersonX then good activity\nUpdate: PersonX was faking to get close to a girl\"\n[output]=\"weakener\"\n\n",
        "expected_content": "\"Premise\", \"Hypothesis\", and \"Update\"",
        "optional_list": [            
        "input",
        "output",
        "\n\n",
        "\\_\\_"
    ],
    "metric": "exact_match",
    "labels": ["strengthener", "weakener"]
    },
    {
        "task_instruction": "In this task, you are given a premise, a hypothesis, and an update. The premise sentence describes a real-world situation and is always assumed to be true. The hypothesis sentence describes an assumption or inference that you might make about that situation having read the premise. The update provides additional information about the situation that might weaken or strengthen the hypothesis. A weakener is a statement that weakens the hypothesis, that is, it makes you much less likely to believe the hypothesis is true. A strengthener is a statement that strengthens the hypothesis, that is, it makes you much more likely to believe the hypothesis is true. Your task is to answer with 'strengthener' or 'weakener' if the update strengthens or weakens the hypothesis, respectively.",
        "task_name": "task936",
        "examples": "[input]=\"Premise: Old man crafting something in his workshop.\nHypothesis: An old man is working.\nUpdate: The man is serious and is surrounded by workers.\"\n[output]=\"strengthener\"\n\n[input]=\"Premise: Old man crafting something in his workshop.\nHypothesis: An old man is working.\nUpdate: The man is wearing pajamas and is chuckling.\"\n[output]=\"weakener\"\n\n",
        "expected_content": "",
        "optional_list": []
    },
    {
        "task_instruction": "In this task, you are given a hypothesis and an update. The hypothesis sentence is a statement that speaks of a socially normative behavior. In other words, it is a generalizing statement about how we expect people to behave in society. The update provides additional contexts about the situation that might UNDERMINE or SUPPORT the generalization. An undermining context provides a situation that weakens the hypothesis. A supporting context provides a situation that strengthens the generalization. Your task is to output 'strengthener' or 'weakener' if the update supports or undermines the hypothesis, respectively",
        "task_name": "task937",
        "examples": "[input]=\"Hypothesis: You should help your family with funeral expenses.\nUpdate: They have asked you to chip in\"\n[output]=\"strengthener\"\n\n[input]=\"Hypothesis: It's good to protect your property.\nUpdate: you don't care what happens to your property.\"\n[output]=\"weakener\"\n\n[input]=\"Hypothesis: You should help your family with funeral expenses.\nUpdate: You are not financially stable to help out\"\n[output]=\"weakener\"\n\n",
        "expected_content": "\"Hypothesis\" and \"Update\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["strengthener", "weakener"]
    },
    {
        "task_instruction": "In this task, you're given two sentences. Indicate if the first sentence clearly entails the second sentence (i.e., one can conclude the 2nd sentence by reading the 1st one). Indicate your answer with '1' if the first sentence entails the second sentence, otherwise answer with '0'.",
        "task_name": "task1344",
        "examples": "[input]=\"Sentence 1: No Weapons of Mass Destruction Found in Iraq Yet. Sentence 2:Weapons of Mass Destruction Found in Iraq.\"\n[output]=\"0\"\n\n[input]=\"Sentence 1: A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI. Sentence 2: Pope Benedict XVI is the new leader of the Roman Catholic Church.\"\n[output]=\"1\"\n\n[input]=\"Sentence 1: Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients. Sentence 2: Herceptin can be used to treat breast cancer.\"\n[output]=\"1\"\n\n[input]=\"Sentence 1: Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That's about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found. Sentence 2: Three quarters of U.S. illegal immigrants have children.\"\n[output]=\"0\"\n\n",
        "expected_content": "\"Sentence 1\" and \"Sentence 2\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["0", "1"]
    },
    {
        "task_instruction": "In this task, you will be presented with a premise and a hypothesis sentence. Determine whether the hypothesis sentence entails (implies), contradicts (opposes), or is neutral with respect to the given premise sentence. Please answer with \"Contradiction\", \"Neutral\", or \"Entailment\".",
        "task_name": "task1385",
        "examples": "[input]=\"Premise: Lost Moon: The Perilous Voyage of Apollo 13 (published in paperback as Apollo 13), is a non-fiction book first published in 1994 by astronaut James Lovell and journalist Jeffrey Kluger, about the failed April 1970 Apollo 13 lunar landing mission which Lovell commanded. The book is the basis of the 1995 film adaptation \"Apollo 13\", directed by Ron Howard. <sep> Hypothesis: the book wouldnt have happened if we didnt try to go into space\"\n[output]=\"Entailment\"\n\n[input]=\"Premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his \"Aṣṭādhyāyī \". <sep> Hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.\"\n[output]=\"Contradiction\"\n\n[input]=\"Premise: Stephen Williams (born 5 June 1961) is a former Australian rules footballer in the South Australian National Football League, playing for the Port Adelaide Magpies and is currently an assistant development coach at Port Adelaide Power and head coach of the Immanuel College first XVIII. <sep> Hypothesis: Stephen Williams quit playing football due to an injury.\"\n[output]=\"Neutral\"\n\n",
        "expected_content": "\"Premise\" and \"Hypothesis\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["Neutral", "Contradiction", "Entailment"]
    },
    {
        "task_instruction": "In this task, you will be presented with a premise and a hypothesis sentence. Determine whether the hypothesis sentence entails (implies), contradicts (opposes), or is neutral with respect to the given premise. Please answer with \"Contradiction\", \"Neutral\", or \"Entailment\".",
        "task_name": "task1386",
        "examples": "[input]=\"Premise: The Washington Nationals are a professional baseball team that has been based in Washington, D.C. since . The Nationals are a member of both the Major League Baseball's (MLB) National League Eastern Division and the National League (NL) itself. Since the 2008 season, the Nationals have played in Nationals Park; from 2005 through , the team played in Robert F. Kennedy Memorial Stadium. <sep> Hypothesis: The Washington Nationals have played in Nationals Park for more than 1000 days.\"\n[output]=\"Entailment\"\n\n[input]=\"Premise: The 1986–87 St. John's Redmen basketball team represented St. John's University during the 1986–87 NCAA Division I men's basketball season. The team was coached by Lou Carnesecca in his nineteenth year at the school. St. John's home games are played at Alumni Hall and Madison Square Garden and the team is a member of the Big East Conference. <sep> Hypothesis: Lou Carnesecca's first season as coach of the St. John's Redmen basketball team was in 1974.\"\n[output]=\"Contradiction\"\n\n[input]=\"Premise: Clear Hearts Grey Flowers is the second full-length and final album by Jack Off Jill. Produced by Chris Vrenna of Nine Inch Nails/Tweaker, it was released in July 2000 on the now-defunct label Risk Records. After \"Clear Hearts, Grey Flowers\" the band formally split up and moved on to establish other projects. <sep> Hypothesis: Risk Records released Jack Off Jill's initial album.\"\n[output]=\"Neutral\"\n\n",
        "expected_content": "\"Premise\" and \"Hypothesis\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["Neutral", "Entailment", "Contradiction"]
    },
    {
        "task_instruction": "In this task, you will be presented with a premise and a hypothesis sentence. Determine whether the hypothesis sentence entails (implies), contradicts (opposes), or is neutral with respect to the given premise sentence. Please answer with \"Contradiction\", \"Neutral\", or \"Entailment\".",
        "task_name": "task1387",
        "examples": "[input]=\"Premise: If you can dream it, you can achieve it — unless you're a goose trying to play a very human game of rugby. In the video above, one bold bird took a chance when it ran onto a rugby field mid-play. Things got dicey when it got into a tussle with another player, but it shook it off and kept right on running. After the play ended, the players escorted the feisty goose off the pitch. It was a risky move, but the crowd chanting its name was well worth it. Video credit: Storyful / Nathan Swarbrick @swazza1990 <sep> Hypothesis: The crowd believed they knew the name of the goose running on the field.\"\n[output]=\"Entailment\"\n\n[input]=\"Premise: Notley hopeful attacks on women politicians will fade in coming year After a politically nasty year that saw women politicians face a barrage of attacks on social media and angry demonstrators shouting to have them put behind bars, Premier Rachel Notley is optimistic there will be a return to civil discourse. In a year-end interview with CBC News, Notley reflected on 2016, a year that saw hateful social-media attacks against MLA Sandra Jansen, and chants of “Lock her up” directed at the premier at an anti-carbon tax rally in Edmonton. <sep> Hypothesis: Notley reflected on the chants of \"lock her up\" from previous to the year 2016.\"\n[output]=\"Contradiction\"\n\n[input]=\"Premise: Mazda Sales (Thailand), a unit of Japanese carmaker Mazda Motor Corp, remains bullish about its sales prospects this year despite falling figures in the first six months, the Bangkok Post reported, citing its President Hidesuke Takesue. (bit.ly/1gr7HMd) ---- NOTE: Reuters has not verified this story and does not vouch for its accuracy. (Bangkok Newsroom; Editing by Prateek Chatterjee) <sep> Hypothesis: Mazda Sales will be fired next week\"\n[output]=\"Neutral\"\n\n",
        "expected_content": "",
        "optional_list": []
    },
    {
        "task_instruction": "In this task, you will be presented with a premise and a hypothesis sentence. Determine whether the hypothesis sentence entails (implies), contradicts (opposes), or is neutral with respect to the given premise. Please answer with \"Contradiction\", \"Neutral\", or \"Entailment\".",
        "task_name": "task1388",
        "examples": "[input]=\"Premise: Ockleton, Morpurgo, Cornelius, Dysart and half a dozen others too drunk to mention. But there was so much coming and going that any one of us could have slipped out, pushed Everett through the window and slipped back again without being noticed. Damn it all we didn't even notice Everett was missing until a porter tripped over him in the quad so anything's theoretically possible. <sep> Hypothesis: Everett was missing\"\n[output]=\"Entailment\"\n\n[input]=\"Premise: I should dearly have liked to know whether they were Europeans or Americans, but I couldn't hear the accents. They appeared to be arguing. I hoped the white men weren't telling him to eliminate all witnesses because I don't believe it would have needed much persuasion. <sep> Hypothesis: eliminating all witnesses would have needed much persuasion\"\n[output]=\"Contradiction\"\n\n[input]=\"Premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. B: Right.  Uh, are you saying you don't think anything should be done in the short term? <sep> Hypothesis: anything should be done in the short term\"\n[output]=\"Neutral\"\n\n",
        "expected_content": "a \"Premise\" and a \"Hypothesis\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["Contradiction", "Entailment", "Neutral"]
    },
    {
        "task_instruction": "In this task, you are given a premise and hypothesis. The task is to classify them into three categories: 'positive' if the hypothesis supports the premise, 'negated' if it opposes the premise, and 'neutral' if it neither supports nor opposes it.",
        "task_name": "task1516",
        "examples": "[input]=\"'Premise : All ten guys that proved to boast were divorcing.','Hypothesis : There are exactly ten guys that proved to boast.'\"\n[output]=\"positive\"\n\n[input]=\"'Premise : All ten reports that can bore some waiter aren't disagreeing with Naomi.','Hypothesis : There are exactly eleven reports that can bore some waiter.'\"\n[output]=\"negated\"\n\n[input]=\"Premise : All ten guys that proved to boast weren't divorcing.','Hypothesis : There are exactly ten senators that proved to boast.'\"\n[output]=\"neutral\"\n\n",
        "expected_content": "a \"Premise\" and a \"Hypothesis\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["neutral", "positive", "negated"]
    },
    {
        "task_instruction": "You are given two sentences. You have to find if there is entailment or agreement of the Hypothesis by the Premise. From the given pair of sentences, you should identify if there is enough information in the Premise to support the claim made in the Hypothesis. The Premise may not exactly be the same as Hypothesis. Your task is to return 'entails' if the premise supports hypothesis else return 'neutral'.",
        "task_name": "task1529",
        "examples": "[input]=\"Premise: Lyme Disease is caused by a bacterium that's transmitted by tick bite, but many infected people don't remember a bite. \n Hypothesis: Lyme disease is caused by bacteria.\"\n[output]=\"entails\"\n\n[input]=\"Premise: Corolla Collective term for all the petals of a flower, these petals may be separate or fused together. \n Hypothesis: All of the petals together are called a corolla.\"\n[output]=\"entails\"\n\n[input]=\"Premise: This can be dangerous to both plants and animals. \n Hypothesis: Nematodes can be a parasite of both.\"\n[output]=\"neutral\"\n\n[input]=\"Premise: The liver is divided into the right lobe and left lobes. \n Hypothesis: The gallbladder is near the right lobe of the liver.\"\n[output]=\"neutral\"\n\n",
        "expected_content": "a \"Premise\" and a \"Hypothesis\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["neutral", "entails"]
    },
    {
        "task_instruction": "In this task, you are given two statements. The task is to output whether a given textual premise, i.e. Statement 2, entails or implies a given scientific fact, i.e. Statement 1. The output should be 'entails' if Statement 2 supports Statement 1 and should be 'neutral' otherwise.",
        "task_name": "task1554",
        "examples": "[input]=\"Sentence 1: The sum of all chemical reactions that take place within an organism is known as metabolism. Sentence 2: Metabolism is the sum total of all chemical reactions performed by an organism.\"\n[output]=\"entails\"\n\n[input]=\"Sentence 1: The endocrine system produces most of the hormones that regulate body functions. Sentence 2: Your endocrine glands produce hormones that control all your body functions.\"\n[output]=\"entails\"\n\n[input]=\"Sentence 1: Warm and humid temperature and moisture conditions describe an air mass that originates over the Atlantic ocean near the equator. Sentence 2: Maritime tropical air Warm, humid air mass that forms over tropical and subtropical oceans.\"\n[output]=\"neutral\"\n\n",
        "expected_content": "\"Sentence 1\" and \"Sentence 2\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["entails", "neutral"]
    },
    {
        "task_instruction": "In this task, given 2 input sentences, you must classify the relation between them. If the second sentence has a similar meaning to that of the first sentence then the output is ' B_entails_A', if the second sentence has the opposite meaning to the first sentence then it is classified as ' B_contradicts_A'. If you cannot clearly ascertain agreement/disagreement between the two sentences, the label is ' B_neutral_A'.",
        "task_name": "task1615",
        "examples": "[input]=\"sentence_A: man is wearing a hard hat and dancing. sentence_B: There is no man with a hard hat dancing.\"\n[output]=\"B_contradicts_A\"\n\n[input]=\"sentence_A: A baby is crying. sentence_B: A man is exercising.\"\n[output]=\"B_neutral_A\"\n\n[input]=\"sentence_A: A tiger is pacing around a cage. sentence_B: A tiger is walking around a cage\"\n[output]=\"B_entails_A\"\n\n",
        "expected_content": "\"sentence_A\" and \"sentence_B\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match"
    },
    {
        "task_instruction": "In this task, you're given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the numbers 0 (entailment), 1 (neutral), or 2(contradiction).",
        "task_name": "task1612",
        "examples": "[input]=\"sentence_A: A dancer is dancing on the stage. sentence_B: A girl is giving dance performance on the dais.\"\n[output]=\"0\"\n\n[input]=\"sentence_A: The crowd is cheering at her dance performance. sentence_B: The group is enjoying while eating food.\"\n[output]=\"1\"\n\n[input]=\"sentence_A: A man is standing and has tears of joy seeing the dance performance. sentence_B: There is no man standing with happiness seeing the dance.\"\n[output]=\"2\"\n\n",
        "expected_content": "\"sentence_A\" and \"sentence_B\"",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "exact_match",
        "labels": ["0", "1", "2"]
    },
    {
        "task_instruction": "Given an abstract, generate a keyword (a noun phrase) that best describes the focus or contribution of the paper. Such keywords can be directly from the given abstract or outside it.",
        "task_name": "task620",
        "examples": "[input]=\"Abstract: Some patients converted from ventricular fibrillation to organized rhythms by defibrillation - trained ambulance technicians(EMT - Ds) will refibrillate before hospital arrival.The authors analyzed 271 cases of ventricular fibrillation managed by EMT - Ds working without paramedic back - up.Of 111 patients initially converted to organized rhythms, 19(17 % ) refibrillated, 11(58 % ) of whom were reconverted to perfusing rhythms, including nine of 11(82 % ) who had spontaneous pulses prior to refibrillation.Among patients initially converted to organized rhythms, hospital admission rates were lower for patients who refibrillated than for patients who did not(53 % versus 76 % , P = NS), although discharge rates were virtually identical(37 % and 35 % , respectively).Scene - to - hospital transport times were not predictively associated with either the frequency of refibrillation or patient outcome.Defibrillation - trained EMTs can effectively manage refibrillation with additional shocks and are not at a significant disadvantage when paramedic back - up is not available.\"\n[output]=\"Ventricular Fibrillation\"\n\n[input]=\"Abstract: Our results suggest that ethylene oxide retention after sterilization is increased in cuprammonium cellulose plate dialyzers containing potting compound. In contrast, cuprammonium cellulose plate dialyzers without potting compound were characterized by a rapid disappearance of retained ethylene oxide after sterilization. Whether these findings explain the low incidence of SARD with cuprammonium cellulose plate dialyzers that do not contain potting material is a matter for continued study and experimentation.\"\n[output]=\"Sterilization\"\n\n",
        "expected_content": "",
        "optional_list": []
    },
    {
        "task_instruction": "In this task you're given a question and you have to paraphrase the question to create the output question while retaining the meaning of the original question.",
        "task_name": "task1345",
        "examples": "[input]=\"What can one do after MBBS?\"\n[output]=\"What do i do after my MBBS ?\"\n\n[input]=\"Which is the best book to study TENSOR for general relativity from basic?\"\n[output]=\"Which is the best book for tensor calculus?\"\n\n[input]=\"What are the coolest Android hacks and tricks you know?\"\n[output]=\"What are some cool hacks for Android phones?\"\n\n[input]=\"Which are the best motivational videos?\"\n[output]=\"What are some of the best motivational clips?\"\n\n",
        "expected_content": "a question",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Given a disfluent sentence, modify the sentence to it to its equivalent fluent form, preserving the meaning of the sentence.",
        "task_name": "task1195",
        "examples": "[input]=\"Who did the Han Chinese want to help the Khitan no I mean the Mongols fight?\"\n[output]=\"Who did the Han Chinese want to help the Mongols fight?\"\n\n[input]=\"What part did no I meant how many chapters have coordinating lead authors?\"\n[output]=\"How many chapters have coordinating lead authors?\"\n\n[input]=\"How did no Who masterminded many terror attacks?\"\n[output]=\"Who masterminded many terror attacks?\"\n\n",
        "expected_content": "a disfluent sentence",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you are given two questions about a domain. Your task is to combine the main subjects of the questions to write a new, natural-sounding question. For example, if the first question is about the tallness of the president and the second question is about his performance at college, the new question can be about his tallness at college. Try to find the main idea of each question, then combine them; you can use different words or make the subjects negative (i.e., ask about shortness instead of tallness) to combine the subjects. The questions are in three domains: presidents, national parks, and dogs. Each question has a keyword indicating its domain. Keywords are \"this national park\", \"this dog breed\", and \"this president\", which will be replaced with the name of an actual president, a national park, or a breed of dog. Hence, in the new question, this keyword should also be used the same way. Do not write unnatural questions. (i.e., would not be a question someone might normally ask about domains). Do not write open-ended or subjective questions. (e.g., questions that can be answered differently by different people.) If you couldn't find the answer to your question from a single Google search, try to write a different question. You do not have to stick with the original question word for word, but you should try to create a question that combines the main subjects of the question.",
        "task_name": "task121",
        "examples": "[input]=\"What college did this president attend? Where did this president meet his wife?\"\n\n[output]=\"Did this president meet his wife in college?\"\n\n[input]=\"Does this dog breed usually weigh 25 pounds or more fully grown? How does this dog breed act around strangers?\"\n\n[output]=\"Is this dog breed a friendly dog who weighs more than 25 lbs?\"\n\n[input]=\"What are the popular tourist spots in this national park? What varieties of trees are in this national park?\"\n\n[output]=\"What are the names of famous trees in this national park?\"\n\n",
        "expected_content": "two short questions about the same domain",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "You're given a fill-in-the-blank question where the answer is PersonX. You need to minimally change the given question so that the answer flips to PersonY. This task typically involves replacing one word i.e., the 'trigger word' with its antonym (e.g., changing from \"sympathetic\" to \"stern\"). You should not change any content in the given question beyond a word or two i.e. the trigger word/phrase. PersonX and PersonY should not be equally likely to fill the blank. For your question, PersonY should be a well-agreed answer to fill in the blank. Your generations should NOT contain potentially explicit, offensive, or adult content. Do not use the names of real people or generic names (e.g., Donald Trump, John Doe, etc.) in your question. Avoid repeating the same style or phrase in generating your modified question e.g. this task can be always solved using a simple negation i.e. by adding not, never, etc. Instead, try to increase the word diversity. Your question must contain at least 15 and at most 30 words. Your question must have at least 70% overlapping words with the given question. You must utilize the given context word while writing the question. Your question must contain only one blank. Make sure that PersonX and PersonY have the same gender. In your question, PersonX and PersonY should be used only ONCE and PersonX should appear earlier than PersonY. Although there are many correct answers, you only need to write one of them.",
        "task_name": "task035",
        "examples": "[input]=\"Context word: upset. \nQuestion: PersonX yelled at PersonY because _ was so upset about the news. \nAnswer: PersonX.\"\n[output]=\"PersonX comforted at PersonY because _ was so upset about the news.\"\n\n[input]=\"Context word: library. \nQuestion: PersonX asked PersonY what time the library closes, because _ had forgotten. \nAnswer: PersonX.\"\n[output]=\"PersonX asked PersonY what time the library closes, but _ had forgotten.\"\n\n[input]=\"Context word: door. \nQuestion: PersonX knocked on PersonY's door, but there was no answer. _ was disappointed. \nAnswer: PersonX.\"\n[output]=\"PersonX knocked on PersonY's door, but there was no answer. _ was out.\"\n\n[input]=\"Context word: step. \nQuestion: PersonX was always ahead of PersonY, as _ walked with a quick step. \nAnswer: PersonX.\"\n[output]=\"PersonX was always behind PersonY, as _ walked with a quick step .\"\n\n[input]=\"Context word: dropped. \nQuestion: When PersonX dropped his ice cream, PersonY giggled, so father gave _ a sympathetic look. \nAnswer: PersonX.\"\n[output]=\"When PersonX dropped his ice cream, PersonY giggled, so father gave _ a stern look.\"\n\n",
        "expected_content": "Context word, Question and Answer",
        "optional_list": [            
        "input",
        "output",
        "\n\n"],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you're given an ambiguous question (which can be answered in more than one way). Your task is to provide one question which clarifies the input question and it has one unique answer, and also provide an answer to the generated question. Generated question and answer should be separated with a new line.",
        "task_name": "task671",
        "examples": "[input]=\"Lucifer how many episodes are in season 3?\"\n[output]=\"How many episodes of season 3 of Lucifer were there, including bonus episodes? \n 26\"\n\n[input]=\"What is the tallest ride at six flags over texas?\"\n[output]=\"What is the tallest roller coaster at six flags over texas 2001? \n The Titan\"\n\n[input]=\"Who won the final hoh big brother 20?\"\n[output]=\"Who won the Final HoH in the American reality show Big Brother 20? \n Kaycee Clark\"\n\n",
        "expected_content": "an ambiguous question (which can be answered in more than one way)",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Paraphrase the given questions to have different wording. Your paraphrased questions should have the same answer as the original question. Try to change the sentence as much as possible using synonyms and/or rearranging the structure of the sentence. The questions are in three domains: presidents, national parks, and dogs. Each question has a keyword indicating its domain. Keywords are \"this national park\", \"this dog breed\", and \"this president\", which will be replaced with the name of an actual president, a national park, or a breed of dog. Hence, in paraphrasing, this keyword should also be used the same way. Do not write questions that compare or involve multiple domains. Do not write open-ended or subjective questions (e.g., questions that can be answered differently by different people.) Make your questions specific and concrete. Your question should have the same type of answer as the original question(e.g., if the question is extractive, the paraphrased question should be extractive as well.)",
        "task_name": "task1562",
        "examples": "[input]=\"Does this dog breed have short legs compared to the rest of its body?\"\n[output]=\"Is the body of this dog breed large compared to its legs?\"\n\n[input]=\"Does this dog breed have short legs compared to the rest of its body?\"\n[output]=\"Are the legs of this dog breed proportionally shorter than its body?\"\n\n[input]=\"Does this dog breed have an average life expectancy range that can be more than 12 years?\"\n[output]=\"Does this dog breed have a lifespan of more than 12 years?\"\n\n",
        "expected_content": "a question",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Convert a disfluent question to a proper question. A disfluent question is a question that has some interruptions in it while framing. A proper question is the correct form of the question without any disfluency.",
        "task_name": "task1622",
        "examples": "[input]=\"Why was uh where was the Rhine regulated with an upper canal?\"\n[output]=\"Where was the Rhine regulated with an upper canal?\"\n\n[input]=\"What kind of committee considered legislation on the development of the Scottish or no make that the Edinburgh Tram Network?\"\n[output]=\"What kind of committee considered legislation on the development of the Edinburgh Tram Network?\"\n\n[input]=\"When degradation no economic inequality is smaller, more waste and pollution is?\"\n[output]=\"When economic inequality is smaller, more waste and pollution is?\"\n\n",
        "expected_content": "a disfluent question",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you're given a fill-in-the-blank question that contains two object names. Additionally, you're given one answer which is one of the objects present in the question. In this task, you need to minimally change the given question so that the answer flips to another object in the question. Remember the question has to be about both objects which are related but different; for example, \"trophy\" and \"suitcase\". This task typically involves replacing one word i.e., the 'trigger word' with its antonym (e.g., changing from \"small\" to \"big\"). You should not change any content in the given question beyond a word or two, i.e., the trigger word/phrase. The expected answer to your question must not be associated with just the trigger word; instead, it should depend on the context present in the question. The expected answer should not be ambiguous. For your question, there should be an agreed upon answer to fill in the blank. Your generations should NOT contain potentially explicit, offensive, or adult content. In your generated question, retain the object names from the input question. Avoid repeating the same style or phrase in generating your modified question e.g. this task can be always solved using a simple negation i.e. by adding not, never, etc. Instead, try to increase the word diversity. Your question must contain at least 15 and at most 30 words. Your question must have at least 70% overlapping words with the given question. You must utilize the given context word while writing the question. Your question must contain only one blank. The two objects should be used ONCE in your question. Here is a list of attributes and associated contrastive words that may help write contrastive trigger words. Note that contrastive trigger words help in flipping the label.\n| Attribute | triggerword | contrastive triggerword | \n| age | old | new | \n| altitude | low | high |\n| area | small | vast | \n| brightness | dark | light | \n| clarity | obscure | clear | \n| cleanness | dirty | clean | \n| complexity | simple | complex | \n| cost | cheap | expensive |\n| density | sparse | dense |\n| depth | shallow | deep |\n| distance | near | far |  \n| electric conductivity | low | high |\n| flexibility | rigid | flexible |\n| granularity | fine | coarse | \n| hardness | soft | hard | \n| length | short | long | \n| magnitude | small | large | \n| mass | small | large | \n| odor | weak | strong | \n| pressure | low | high | \n| resistance | low | high | \n| shape | round | sharp |\n| shape | flat | spiky |\n| size | small | large | \n| sound | quiet | loud | \n| sound pitch | low | high | \n| speed | slow | fast | \n| stability | unstable | stable | \n| strength | weak | strong | \n| temperature | low | high | \n| texture | smooth | rough | \n| thermal conductivity | low | high |\n| thickness | thin | thick | \n| volume | small | large |\n| weight | light | heavy | \n| width | narrow | wide | \n| location | in | out |\n| location | up | down |\n| location | above | below |\n| location | on | off |\n| location | to | from |\n",
        "task_name": "task034",
        "examples": "[input]=\"Context word: fit. \nQuestion: The trophy doesn't fit into the brown suitcase because _ is too large. \nAnswer: trophy.\"\n[output]=\"The trophy doesn't fit into the brown suitcase because _ is too small.\"\n\n[input]=\"Context word: trade. \nQuestion: Grace was happy to trade me her sweater for my jacket. She thinks _ looks dowdy on her. \nAnswer: sweater.\"\n[output]=\"Grace was happy to trade me her sweater for my jacket. She thinks _ looks great on her.\"\n\n[input]=\"Context word: carpet. \nQuestion: While redecorating her home, Sam took out the carpet and replaced it with wood floors. The _ was old. \nAnswer: carpet.\"\n[output]=\"While redecorating her home, Sam took out the carpet and replaced it with wood floors. The _ was trendy.\"\n\n[input]=\"Context word: fit. \nQuestion: The table won't fit through the doorway because the _ is too wide. \nAnswer: table.\"\n[output]=\"The table won't fit through the doorway because the _ is too narrow.\"\n\n[input]=\"Context word: water. \nQuestion: I poured water from the bottle into the cup until the _ was empty. \nAnswer: bottle.\"\n[output]=\"I poured water from the bottle into the cup until the _ was full.\"\n\n[input]=\"Context word: catch. \nQuestion: My meeting started at 4:00 and I needed to catch the train at 4:30, so there wasn't much time. Luckily, the _ was short, so it worked out. \nAnswer: meeting.\"\n[output]=\"My meeting started at 4:00 and I needed to catch the train at 4:30, so there wasn't much time. Luckily, the _ was delayed, so it worked out.\"\n\n[input]=\"Context word: candle. \nQuestion: James wanted to light the candle in the wind when on a date but the _ is too weak. \nAnswer: light.\"\n[output]=\"James wanted to light the candle in the wind when on a date but the _ is too strong.\"\n\n",
        "expected_content": "Context word, Question and Answer",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Generate an appropriate title for the given text. The generated title must be short and include the main topic of the text. The preferred titles are under fifteen words.",
        "task_name": "task1356",
        "examples": "[input]=\"But Eluned Morgan conceded that it would be \"difficult for us to stop\" from a legal point of view. Her comments were criticised by a Labour AM. Alun Davies said threatening legal action \"sounds like the last breath before you're thrown out of the pub\". Mr Davies said he was not convinced the Welsh Government would \"have a leg to stand on\" in trying to shape international trade deals after Brexit. Following Donald Trump's comments during last week's trade visit that the NHS would be \"on the table\" in any future trade talks between the UK and the USA, Eluned Morgan said there was \"absolutely no prospect whatsoever of us allowing the Welsh NHS to be part of any negotiation.\" The US President then rowed back on his initial comments following criticism from a number of MPs. Asked about her response to President Trump's remarks as she gave evidence to the Assembly's Brexit committee on Monday, Ms Morgan said \"legally, it would be difficult for us to stop because we don't have a veto over trade\". \"Politically, I think it's extremely unlikely to happen,\" the international relations and the Welsh language minister said. \"They [the UK Government] should not be concluding any trade agreements without consulting us where we have the power.\" Ms Morgan explained that UK and Welsh government officials are working on an agreement or 'concordat' for how future trade deals are negotiated. During a robust exchange, the Labour AM Alun Davies said: \"I want something which is in law to which I can hold you to account and which colleagues in Westminster can hold the UK Government to account. \"The argument we'll make life difficult for them, it sounds alright on the street, but it's not the reality of intergovernmental relations.\" \"The United Kingdom has to find a way of functioning. \"At the moment, your answers aren't giving me any confidence that there is that structure in place because, if the Welsh Government's argument is, 'we'll see you in court', it's not a very impressive argument either for the continuation of the structure of United Kingdom as a state or the commitment of the government within the United Kingdom to actually work together,\" he added. Responding to the criticism, Ms Morgan said: \"Is the current intergovernmental structure adequate? \"Absolutely not... and it's not just in relation to trade, it's in relation to almost every aspect of government policy. So, that infrastructure needs to be built.\"\"\n[output]=\"NHS Wales: Court action if trade deals affect service?\"\n\n[input]=\"By Jon Welch and Paul MoseleyBBC News Details of health problems, family bereavements and personal issues were sent by the University of East Anglia (UEA) in Norwich to 298 students. Megan Baynes, 23, said she felt \"sick and horrified\" when she realised her details had been shared. The UEA apologised \"unreservedly\" and said an inquiry had begun. The email contained a spreadsheet listing 172 names and details extenuating circumstances in which extensions and other academic concessions were granted to 42 students. 'Felt sick' It was sent to nearly 300 undergraduates, including Ms Baynes, a former editor of student newspaper Concrete. She is currently awaiting the results of her American Literature and Creative Writing degree, and had been granted extensions for coursework because of an illness suffered by a family member. \"I felt sick at seeing my personal situation written in a spreadsheet, and then seemingly sent to everyone on my course,\" she said. \"My situation was not the worst on there but there are some on there that are so personal. There are people I know and I feel so awful for them and can't imagine how they are feeling.\" Theo Antoniou Phillips, UEA Students' Union undergraduate education officer, said: \"This is a shocking and utterly unacceptable data breach that should never have happened.\" Jo Swo, the union's welfare, community and diversity officer, said: \"Given the university is supposed to be making mental health a priority, this is a real slap in the face to students who have sought support.\" In a statement, a UEA spokeswoman said: \"An email was mistakenly sent to 298 American Studies undergraduates this morning containing details of 42 students with extenuating circumstances. \"This clearly should not have happened and the university apologises unreservedly. The university has launched an urgent enquiry and is contacting all affected students to offer support. \"Anyone needing support should call 01603 592761. The university is informing the ICO (Information Commissioner's Office).\" The ICO has been contacted for comment.\"\n[output]=\"University of East Anglia in students' personal data breach\"\n\n[input]=\"Media reports say Stanislav Bogdanovich and Alexandra Vernigora - also a top chess player - were found with balloons containing the gas, nitrous oxide. The gas is inhaled using a balloon. Russian investigators reported the deaths, without naming the pair, and said there were no signs of foul play. Bogdanovich was a speed chess champion. Vernigora was also a professional chess player and was studying at Moscow State University. The Ukrainian sports website sport.ua says Bogdanovich was a grandmaster from Odessa who won the Ukrainian Under-18 championship and various chess awards at international tournaments. Russian chess website chess-news.ru says that in 2015 he was rated eighth in the world for speed (blitz) chess. Reports say Bogdanovich drew much criticism recently for representing Russia in an internet chess match against Ukraine, which he won. Sport.ua quotes a Facebook post from him (in Russian) about that, in which he argued that playing for Russia was good for business, that he was living as a guest in Russia and being treated well, and this was his small contribution to ending the Russia-Ukraine conflict. Nitrous oxide was first used as an anaesthetic in 1844, but is now being used as a recreational drug and has been linked to a number of deaths. It can also cause breathing difficulties, dangerously increased heart rate and burns.\"\n[output]=\"Young Ukraine chess couple 'killed by laughing gas'\"\n\n",
        "expected_content": "some text on a same topic",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.",
        "task_name": "task1540",
        "examples": "[input]=\"We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.\"\n[output]=\"A Neural Local Coherence Model\"\n\n[input]=\"We present four methods for recovering the epipolar geometry from images of smooth surfaces. In the existing methods for recovering epipolar geometry corresponding feature points are used that cannot be found in such images. The first method is based on finding corresponding characteristic points created by illumination (ICPM illumination characteristic points method). The second method is based on correspondent tangency points created by tangents from epipoles to outline of smooth bodies (OTPM outline tangent points method). These two methods are exact and give correct results for real images, because positions of the corresponding illumination characteristic points and corresponding outline are known with small errors. But the second method is limited either to special type of scenes or to restricted camera motion. We also consider two more methods which are termed CCPM (curve characteristic points method, green curves are used for this method on Figures) and CTPM (curve tangent points method, red curves are used for this method on Figures), for searching epipolar geometry for images of smooth bodies based on a set of level curves (isophoto curves) with a constant illumination intensity. The CCPM method is based on searching correspondent points on isophoto curves with the help of correlation of curvatures between these lines. The CTPM method is based on property of the tangential to isophoto curve epipolar line to map into the tangential to correspondent isophoto curves epipolar line. The standard method termed SM (standard method, blue curves are used for this method on Figures) and based on knowledge of pairs of the almost exact correspondent points, has been used for testing of these two methods. The main technical contributions of our CCPM method are following. The first of them consists of bounding the search space for epipole locations. On the face of it, this space is infinite and unbounded. We suggest a method to partition the infinite plane into a finite number of regions. This partition is based on the desired accuracy and maintains properties that yield an efficient search over the infinite plane. The second is an efficient method for finding correspondence between points of two closed isophoto curves and finding homography, mapping between these two isophoto curves. Then this homography is corrected for all possible epipole positions with the help of evaluation function. A finite subset of solution is chosen from the full set given by all possible epipole positions. This subset includes fundamental matrices giving local minimums of evaluating function close to global minimum. Epipoles of this subset lie almost on straight line directed parallel to parallax shift. CTPM method was used to find the best solution from this subset. Our method is applicable to any pair of images of smooth objects taken under perspective projection models, as long as assumption of the constant brightness is taken for granted. The methods have\"\n[output]=\"Recovering Epipolar Geometry from Images of Smooth Surfaces\"\n\n[input]=\"Word sense disambiguation (WSD) is a problem in the field of computational linguistics given as finding the intended sense of a word (or a set of words) when it is activated within a certain context. WSD was recently addressed as a combinatorial optimization problem in which the goal is to find a sequence of senses that maximize the semantic relatedness among the target words. In this article, a novel algorithm for solving the WSD problem called D-Bees is proposed which is inspired by bee colony optimization (BCO) where artificial bee agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a standard dataset (SemEval 2007 coarse-grained English all-words task corpus) and is compared to simulated annealing, genetic algorithms, and two ant colony optimization techniques (ACO). It will be observed that the BCO and ACO approaches are on par. 1 ar X iv :1 40 5. 14 06 v1 [ cs .C L ] 6 M ay 2 01 4\"\n[output]=\"D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving Word Sense Disambiguation\"\n\n",
        "expected_content": "a part of an article",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you are given a summary for US Congressional and California state bill, your task is to generate a Title for this bill. The preferred titles are under forty words and mention the purpose of the bill.",
        "task_name": "task1659",
        "examples": "[input]=\"Amends the Water Resources Development Act of 1999 to: (1) authorize appropriations for FY 1999 through 2009 for implementation of a long-term resource monitoring program with respect to the Upper Mississippi River Environmental Management Program (currently, such funding is designated for a program for the planning, construction, and evaluation of measures for fish and wildlife habitat rehabilitation and enhancement); (2) authorize the Secretary of the Army to carry out modifications to the navigation project for the Delaware River, Pennsylvania and Delaware, if such project as modified is technically sound, environmentally (currently, economically) acceptable, and economically justified; (3) subject certain previously deauthorized water resources development projects to the seven-year limitation governing project deauthorizations under the Act, with the exception of such a project for Indian River County, Florida; (4) except from a certain schedule of the non-Federal cost of the periodic nourishment of shore protection projects constructed after December 31, 1999, those projects for which a District Engineer's Report has been completed by such date; (5) require that the project cooperation agreement for the Comite River Diversion Project for flood control include a provision that specifies that any reduction in the non-Federal share that results from certain modifications be credited toward the share of project costs to be paid by the Amite River Basin Drainage and Water Conservation District; (6) allow the Secretary to provide additional compensation to Chesapeake City, Maryland (currently, to the City of Chesapeake, Maryland) for damage to its water supply resulting from the Chesapeake and Delaware Canal Project; (7) provide for the submission of certain reports on water resources development projects by the Secretary, notwithstanding Federal reporting termination provisions; and (8) authorize and provide for an authorization of appropriations for the existing program for the safety and operations expenses of the Federal Railroad Administration, and make available for obligation funds currently appropriated for such program.\"\n[output]=\"To make technical corrections to the Water Resources Development Act of 1999.\"\n\n[input]=\"Maritime Regulatory Reform Act of 1994 - Amends Federal maritime law to authorize the Secretary of the department in which the Coast Guard operates, in order to implement the International Management Code for the Safe Operation of Ships and for Pollution Prevention adopted by the International Maritime Organization and to establish alternative compliance programs, to:  (1) prescribe regulations governing the U.S. merchant marine, merchant marine personnel, and shore-based management of vessels that affect the safety of vessels and personnel and marine pollution prevention; and (2) establish optional regulatory requirements commensurate with the level of quality control adopted by the shipowner or operator, provided that an equivalent level of safety is maintained. \n(Sec. 3) Authorizes the Secretary to utilize reports, documents, and certificates issued by persons who may be relied upon with regard to marine safety, security, and environmental protection. \n(Sec. 4) Authorizes the Secretary to accept approvals of fire and life safety equipment and materials by foreign governments which utilize design and testing standards that meet the requirements of the International Convention for the Safety of Life at Sea, and its associated International Maritime Organization guidance documents. \n(Sec. 5) Requires each in service small passenger vessel carrying more than 12 passengers on an international voyage to be inspected annually.  Requires any other vessel to be inspected at least once every five years (currently, every two years).  Extends from two to five years the effective validation period of certificates of inspection issued to U.S. vessels that carry oil or hazardous material in bulk. \n(Sec. 6) Requires the owner or individual in charge of a vessel to submit certain inspection related notices to the Secretary at least 30 days (currently, at least 30 days but not more than 60 days) before the current certificate of inspection issued to a vessel expires. \n(Sec. 7) Revises provisions regarding the recognition of U.S. classification societies.\"\n[output]=\"Maritime Regulatory Reform Act of 1994\"\n\n[input]=\"Small Business Modernization Act of 2004 - Amends the Internal Revenue Code to: (1) terminate subchapter S corporation elections after 2004 and subchapter S status after 2014 and to allow privately-held domestic corporations, in lieu of electing subchapter S treatment, to elect to be treated as partnerships for tax purposes; (2) set forth rules for the tax treatment of former subchapter S corporations electing partnership status; and (3) exclude from net earnings from self-employment partnership income attributable to capital.\"\n[output]=\"To amend the Internal Revenue Code of 1986 to provide for unified income taxation with respect to pass-thru entities.\"\n\n",
        "expected_content": "a summary for US Congressional and California state bill",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you have to generate the title of the recipe given its required ingredients and directions.",
        "task_name": "task569",
        "examples": "[input]=\"ingredients: '1 cup minced onion', '1 1/2 tablespoons lemon juice', '3/4 teaspoon Hungarian paprika', '3/4 teaspoon ground cayenne pepper', '1/4 teaspoon salt', <sep> directions: 'Rinse onions in sieve under cold running water; drain.', 'Mix together ingredients in a small bowl; allow flavors to meld over 1 hour period before serving.', 'Serve as table condiment with Indian foods, samosas, pakoras, along with other chutneys.'\"\n[output]=\"Indian Hot Onion Relish\"\n\n[input]=\"ingredients: '2 sheets frozen puff pastry, thawed', '4 oz semi-dried tomatoes, chopped', '3 oz seeded black olives', '4 oz soft goat cheese, crumbled', '1/2  red onion, sliced', '1/4 cup fresh torn basil leaves', '1  egg, beaten lightly', <sep> directions: 'Preheat oven to 400\\u00b0F. Line a baking tray with parchment paper.', 'Cut a 6 x 9 1/2 inch rectangle from 1 sheet of puff pastry. Place on baking tray and top with tomatoes, olives, cheese, onion and basil leaves.', 'Cut a 7 x 9 1/2 inch rectangle from remaining sheet of puff pastry. Score in a diamond pattern then place on top of filling, pressing edges to seal. Brush with egg then bake for 20 mins.'\"\n[output]=\"Tomato, Olive And Goat Cheese Tart\"\n\n",
        "expected_content": "ingredients and directions of a recipe",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Given two noun phrases (arguments) and relationship between them, form a sentence that expresses theses arguments with the given relationship.",
        "task_name": "task677",
        "examples": "[input]=\"Relationship: 'knock', Argument/Subject 1: 'someone', Argument/Subject 2: 'it'\"\n[output]=\"The mailbox was bent and broken and looked like someone had knocked it over on purpose.\"\n\n[input]=\"Relationship: 'talk', Argument/Subject 1: 'you', Argument/Subject 2: 'me'\"\n[output]=\"Are you seriously going to talk about me that way?\"\n\n[input]=\"Relationship: 'read', Argument/Subject 1: 'I', Argument/Subject 2: 'book'\"\n[output]=\"To expand my knowledge on the topic, I read a book.\"\n\n",
        "expected_content": "two noun phrases (arguments) and relationship between them",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "Given an entity, a before event, an after event, and an attribute related to the entity, generate a sentence as output. Your sentence should show the changes in the attribute of the entity.",
        "task_name": "task1631",
        "examples": "[input]=\"entity: arm \nbefore: free \nafter: grasped \nattr: state\"\n[output]=\"state of arm was free before and grasped afterwards\"\n\n[input]=\"entity: person \nbefore: standing \nafter: in air \nattr: location\"\n[output]=\"location of person was standing before and in air afterwards.\"\n\n[input]=\"entity: mixture \nbefore: chunky \nafter: smoother \nattr: texture\"\n[output]=\"texture of mixture was chunky before and smoother afterwards.\"\n\n",
        "expected_content": "an entity, a before event, an after event and an attribute related to the entity",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "The task is to write a full sentence or two using all of the information given. The sentence(s) will be a brief review of a restaurant. Use all of the information provided.",
        "task_name": "task1598",
        "examples": "[input]=\"name[xname], cuisine[Fast Food], rating[average], familyFriendly[yes], near[xnear]\"\n[output]=\"Located near xnear, xname serves Fast food and is child friendly. its customer rating is: average.\"\n\n[input]=\"name[xname], cuisine[Chinese], price[less than £20]\"\n[output]=\"xname, Chinese restaurant,  prices from £19.99\"\n\n[input]=\"name[xname], recommend[yes], cuisine[Vietnamese], qual[good], price[affordable]\"\n[output]=\"I would suggest xname because it's affordable and a Vietnamese restaurant with nice food!\"\n\n",
        "expected_content": "some information about a restaurant",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, we ask you convert a data table of restaurant descriptions into fluent natural-sounding English sentences. The input is a string of key-value pairs; the output should be a natural and grammatical English sentence containing all the information from the input.",
        "task_name": "task957",
        "examples": "[input]=\"name[Aromi], eatType[restaurant], food[English], area[city centre]\"\n[output]=\"Aromi is an English restaurant in the city centre.\"\n\n[input]=\"name[The Rice Boat], food[Indian], priceRange[cheap], customer rating[5 out of 5], area[city centre], familyFriendly[yes], near[Express by Holiday Inn]\"\n[output]=\"The Rice Boat is a cheap Indian restaurant in the center of the city near Express by Holiday Inn. It is family friendly and has the highest customer rating.\"\n\n",
        "expected_content": "a data table of restaurant descriptions",
        "optional_list": [
            "input",
            "output",
            "\n\n",
            "\\_\\_"
        ],
        "metric": "rouge"
    },
    {
        "task_instruction": "In this task, you will be shown an incorrect English sentence. You need to generate a corrected form of the input sentence.",
        "task_name": "task1557",
        "examples": "[input]=\"The car's wheel are loose.\"\n[output]=\"The car's wheel is loose.\"\n\n[input]=\"The value of the car is very less.\"\n[output]=\"The value of the car is much less.\"\n\n[input]=\"To day is sunny.\"\n[output]=\"Today is sunny.\"\n\n",
        "expected_content": "an incorrect English sentence",
        "optional_list": [
            "input",
            "output",
            "\n\n"
        ],
        "metric":"rouge"
    },
    {
        "task_instruction": "You are given data in tabular format and you need to generate a passage with content in plain text format based on the information present in the table.",
        "task_name": "task760",
        "examples": "[input]=\"Table: ['Pick', 'Player', 'Team', 'Position', 'School']. ['1', 'Ben McDonald', 'Baltimore Orioles', 'RHP', 'Louisiana State University']. ['2', 'Tyler Houston', 'Atlanta Braves', 'C', 'Valley HS (Las Vegas, NV)']. ['3', 'Roger Salkeld', 'Seattle Mariners', 'RHP', 'Saugus (CA) HS']. ['4', 'Jeff Jackson', 'Philadelphia Phillies', 'OF', 'Simeon HS (Chicago, IL)']. ['5', 'Donald Harris', 'Texas Rangers', 'OF', 'Texas Tech University']. ['6', 'Paul Coleman', 'Saint Louis Cardinals', 'OF', 'Frankston (TX) HS']. ['7', 'Frank Thomas', 'Chicago White Sox', '1B', 'Auburn University']. ['8', 'Earl Cunningham', 'Chicago Cubs', 'OF', 'Lancaster (SC) HS']. ['9', 'Kyle Abbott', 'California Angels', 'LHP', 'Long Beach State University']. ['10', 'Charles Johnson', 'Montreal Expos', 'C', 'Westwood HS (Fort Pierce, FL)']. ['11', 'Calvin Murray', 'Cleveland Indians', '3B', 'W.T. White High School (Dallas, TX)']. ['12', 'Jeff Juden', 'Houston Astros', 'RHP', 'Salem (MA) HS']. ['13', 'Brent Mayne', 'Kansas City Royals', 'C', 'Cal State Fullerton']. ['14', 'Steve Hosey', 'San Francisco Giants', 'OF', 'Fresno State University']. ['15', 'Kiki Jones', 'Los Angeles Dodgers', 'RHP', 'Hillsborough HS (Tampa, FL)']. ['16', 'Greg Blosser', 'Boston Red Sox', 'OF', 'Sarasota (FL) HS']. ['17', 'Cal Eldred', 'Milwaukee Brewers', 'RHP', 'University of Iowa']. ['18', 'Willie Greene', 'Pittsburgh Pirates', 'SS', 'Jones County HS (Gray, GA)']. ['19', 'Eddie Zosky', 'Toronto Blue Jays', 'SS', 'Fresno State University']. ['20', 'Scott Bryant', 'Cincinnati Reds', 'OF', 'University of Texas']. ['21', 'Greg Gohr', 'Detroit Tigers', 'RHP', 'Santa Clara University']. ['22', 'Tom Goodwin', 'Los Angeles Dodgers', 'OF', 'Fresno State University']. ['23', 'Mo Vaughn', 'Boston Red Sox', '1B', 'Seton Hall University']. ['24', 'Alan Zinter', 'New York Mets', 'C', 'University of Arizona']. ['25', 'Chuck Knoblauch', 'Minnesota Twins', '2B', 'Texas A&M University']. ['26', 'Scott Burrell', 'Seattle Mariners', 'RHP', 'Hamden (CT) HS']\"\n[output]=\"Passage: The player named Ben McDonald, plays as Pick-1.Ben McDonald plays for the team Baltimore Orioles, at position RHP. Ben McDonald belongs to the school Louisiana State University. The player named Tyler Houston, plays as Pick-2.Tyler Houston plays for the team Atlanta Braves, at position C. Tyler Houston belongs to the school Valley HS (Las Vegas, NV). The player named Roger Salkeld, plays as Pick-3.Roger Salkeld plays for the team Seattle Mariners, at position RHP. Roger Salkeld belongs to the school Saugus (CA) HS. The player named Jeff Jackson, plays as Pick-4.Jeff Jackson plays for the team Philadelphia Phillies, at position OF. Jeff Jackson belongs to the school Simeon HS (Chicago, IL). The player named Donald Harris, plays as Pick-5.Donald Harris plays for the team Texas Rangers, at position OF. Donald Harris belongs to the school Texas Tech University. The player named Paul Coleman, plays as Pick-6.Paul Coleman plays for the team Saint Louis Cardinals, at position OF. Paul Coleman belongs to the school Frankston (TX) HS. The player named Frank Thomas, plays as Pick-7.Frank Thomas plays for the team Chicago White Sox, at position 1B. Frank Thomas belongs to the school Auburn University. The player named Earl Cunningham, plays as Pick-8.Earl Cunningham plays for the team Chicago Cubs, at position OF. Earl Cunningham belongs to the school Lancaster (SC) HS. The player named Kyle Abbott, plays as Pick-9.Kyle Abbott plays for the team California Angels, at position LHP. Kyle Abbott belongs to the school Long Beach State University. The player named Charles Johnson, plays as Pick-10.Charles Johnson plays for the team Montreal Expos, at position C. Charles Johnson belongs to the school Westwood HS (Fort Pierce, FL). The player named Calvin Murray, plays as Pick-11.Calvin Murray plays for the team Cleveland Indians, at position 3B. Calvin Murray belongs to the school W.T. White High School (Dallas, TX). The player named Jeff Juden, plays as Pick-12.Jeff Juden plays for the team Houston Astros, at position RHP. Jeff Juden belongs to the school Salem (MA) HS. The player named Brent Mayne, plays as Pick-13.Brent Mayne plays for the team Kansas City Royals, at position C. Brent Mayne belongs to the school Cal State Fullerton. The player named Steve Hosey, plays as Pick-14.Steve Hosey plays for the team San Francisco Giants, at position OF. Steve Hosey belongs to the school Fresno State University. The player named Kiki Jones, plays as Pick-15.Kiki Jones plays for the team Los Angeles Dodgers, at position RHP. Kiki Jones belongs to the school Hillsborough HS (Tampa, FL). The player named Greg Blosser, plays as Pick-16.Greg Blosser plays for the team Boston Red Sox, at position OF. Greg Blosser belongs to the school Sarasota (FL) HS. The player named Cal Eldred, plays as Pick-17.Cal Eldred plays for the team Milwaukee Brewers, at position RHP. Cal Eldred belongs to the school University of Iowa. The player named Willie Greene, plays as Pick-18.Willie Greene plays for the team Pittsburgh Pirates, at position SS. Willie Greene belongs to the school Jones County HS (Gray, GA). The player named Eddie Zosky, plays as Pick-19.Eddie Zosky plays for the team Toronto Blue Jays, at position SS. Eddie Zosky belongs to the school Fresno State University. The player named Scott Bryant, plays as Pick-20.Scott Bryant plays for the team Cincinnati Reds, at position OF. Scott Bryant belongs to the school University of Texas. The player named Greg Gohr, plays as Pick-21.Greg Gohr plays for the team Detroit Tigers, at position RHP. Greg Gohr belongs to the school Santa Clara University. The player named Tom Goodwin, plays as Pick-22.Tom Goodwin plays for the team Los Angeles Dodgers, at position OF. Tom Goodwin belongs to the school Fresno State University. The player named Mo Vaughn, plays as Pick-23.Mo Vaughn plays for the team Boston Red Sox, at position 1B. Mo Vaughn belongs to the school Seton Hall University. The player named Alan Zinter, plays as Pick-24.Alan Zinter plays for the team New York Mets, at position C. Alan Zinter belongs to the school University of Arizona. The player named Chuck Knoblauch, plays as Pick-25.Chuck Knoblauch plays for the team Minnesota Twins, at position 2B. Chuck Knoblauch belongs to the school Texas A&M University. The player named Scott Burrell, plays as Pick-26.Scott Burrell plays for the team Seattle Mariners, at position RHP. Scott Burrell belongs to the school Hamden (CT) HS.\"\n\n[input]=\"Table: ['Rank','Name', 'Nationality', 'Time (hand)', 'Notes']. ['','Tommy Green', 'Great Britain', '4:50:10', 'OR']. ['', 'Janis Dalins', 'Latvia', '4:57:20', '']. ['', 'Ugo Frigerio', 'Italy', '4:59:06', '']. ['4', 'Karl Hahnel', 'Germany', '5:06:06', '']. ['5', 'Ettore Rivolta', 'Italy', '5:07:39', '']. ['6', 'Paul Sievert', 'Germany', '5:16:41', '']. ['7', 'Henri Quintric', 'France', '5:27:25', '']. ['8', 'Ernie Crosbie', 'United States', '5:28:02', '']. ['9', 'Bill Chisholm', 'United States', '5:51:00', '']. ['10', 'Alfred Maasik', 'Estonia', '6:19:00', '']. ['', 'Henry Cieman', 'Canada', '', 'DNF']. ['', 'John Moralis', 'Greece', '', 'DNF']. ['', 'Francesco Pretti', 'Italy', '', 'DNF']. ['', 'Arthur Tell Schwab', 'Switzerland', '', 'DNF']. ['', 'Harry Hinkel', 'United States', '', 'DNF']\"\n[output]=\"Passage:  The Great Britain athlete named Tommy Green is ranked  globally. Tommy Green completed the marathon in 4:50:10 seconds. The Latvia athlete named Janis Dalins is ranked  globally. Janis Dalins completed the marathon in 4:57:20 seconds. The Italy athlete named Ugo Frigerio is ranked  globally. Ugo Frigerio completed the marathon in 4:59:06 seconds. The Germany athlete named Karl Hahnel is ranked 4 globally. Karl Hahnel completed the marathon in 5:06:06 seconds. The Italy athlete named Ettore Rivolta is ranked 5 globally. Ettore Rivolta completed the marathon in 5:07:39 seconds. The Germany athlete named Paul Sievert is ranked 6 globally. Paul Sievert completed the marathon in 5:16:41 seconds. The France athlete named Henri Quintric is ranked 7 globally. Henri Quintric completed the marathon in 5:27:25 seconds. The United States athlete named Ernie Crosbie is ranked 8 globally. Ernie Crosbie completed the marathon in 5:28:02 seconds. The United States athlete named Bill Chisholm is ranked 9 globally. Bill Chisholm completed the marathon in 5:51:00 seconds. The Estonia athlete named Alfred Maasik is ranked 10 globally. Alfred Maasik completed the marathon in 6:19:00 seconds. The Canada athlete named Henry Cieman is ranked  globally. Henry Cieman completed the marathon in  seconds. The Greece athlete named John Moralis is ranked  globally. John Moralis completed the marathon in  seconds. The Italy athlete named Francesco Pretti is ranked  globally. Francesco Pretti completed the marathon in  seconds. The Switzerland athlete named Arthur Tell Schwab is ranked  globally. Arthur Tell Schwab completed the marathon in  seconds. The United States athlete named Harry Hinkel is ranked  globally. Harry Hinkel completed the marathon in  seconds.\"\n\n[input]=\"Table: ['Year', 'Miss Northern Ireland', 'Hometown', 'Placement at Miss World', 'Notes']. ['2011', 'Finola Guinnane', 'Drumbo', 'Non-Finalist', 'Top 20 of Beach Beauty and Top 77 of Beauty with a Purpose at Miss World 2011']. ['2012', 'Tiffany Brien', 'Belfast', 'Top 30', 'Top 10 of Beach Fashion and 1st runner-up of Sports & Fitness at Miss World 2012']. ['2010', 'Lori Moore', 'Belfast', 'Top 25', 'Winner of Sports at Miss World 2010']. ['2009', 'Cherie Gardiner', 'Bangor', 'Non-Finalist', '']. ['2008', 'Judith Wilson', 'Enniskillen', 'Non-Finalist', 'Top 19 of Talent at Miss World 2008']. ['2007', 'Melissa Patton', 'Belfast', 'Non-Finalist', '']. ['2006', 'Catherine Jean Milligan', 'Newtownards', 'Top 17', 'Winner of Miss Talent at Miss World 2006']. ['2005', 'Lucy Evangelista', 'Portglenone', 'Top 15', 'Later Miss United Kingdom 2005 and Miss Universe United Kingdom 2005 2nd runner-up']. ['2004', 'Kirsty Anne Gabriel Stewart', 'Enniskillen', 'Non-Finalist', '']. ['2003', 'Diana Sayers', 'Belfast', 'Non-Finalist', '']. ['2002', 'Gayle Williamson', 'Lurgan', 'Non-Finalist', 'Later Miss United Kingdom 2002']. ['2001', 'Angela McCarthy', 'Belfast', 'Non-Finalist', '']. ['2000', 'Julie Lee-Ann Martin', 'Belfast', 'Non-Finalist', '']\"\n[output]=\"Passage:  Tiffany Brien from Belfast was Miss Northern Ireland in the year 2012. Tiffany Brien was placed as a Top 30 at Miss World. Tiffany Brien was Top 10 of Beach Fashion and 1st runner-up of Sports & Fitness at Miss World 2012. Finola Guinnane from Drumbo was Miss Northern Ireland in the year 2011. Finola Guinnane was placed as a Non-Finalist at Miss World. Finola Guinnane was Top 20 of Beach Beauty and Top 77 of Beauty with a Purpose at Miss World 2011. Lori Moore from Belfast was Miss Northern Ireland in the year 2010. Lori Moore was placed as a Top 25 at Miss World. Lori Moore was Winner of Sports at Miss World 2010. Cherie Gardiner from Bangor was Miss Northern Ireland in the year 2009. Cherie Gardiner was placed as a Non-Finalist at Miss World. Cherie Gardiner was . Judith Wilson from Enniskillen was Miss Northern Ireland in the year 2008. Judith Wilson was placed as a Non-Finalist at Miss World. Judith Wilson was Top 19 of Talent at Miss World 2008. Melissa Patton from Belfast was Miss Northern Ireland in the year 2007. Melissa Patton was placed as a Non-Finalist at Miss World. Melissa Patton was . Catherine Jean Milligan from Newtownards was Miss Northern Ireland in the year 2006. Catherine Jean Milligan was placed as a Top 17 at Miss World. Catherine Jean Milligan was Winner of Miss Talent at Miss World 2006. Lucy Evangelista from Portglenone was Miss Northern Ireland in the year 2005. Lucy Evangelista was placed as a Top 15 at Miss World. Lucy Evangelista was Later Miss United Kingdom 2005 and Miss Universe United Kingdom 2005 2nd runner-up. Kirsty Anne Gabriel Stewart from Enniskillen was Miss Northern Ireland in the year 2004. Kirsty Anne Gabriel Stewart was placed as a Non-Finalist at Miss World. Kirsty Anne Gabriel Stewart was . Diana Sayers from Belfast was Miss Northern Ireland in the year 2003. Diana Sayers was placed as a Non-Finalist at Miss World. Diana Sayers was . Gayle Williamson from Lurgan was Miss Northern Ireland in the year 2002. Gayle Williamson was placed as a Non-Finalist at Miss World. Gayle Williamson was Later Miss United Kingdom 2002. Angela McCarthy from Belfast was Miss Northern Ireland in the year 2001. Angela McCarthy was placed as a Non-Finalist at Miss World. Angela McCarthy was . Julie Lee-Ann Martin from Belfast was Miss Northern Ireland in the year 2000. Julie Lee-Ann Martin was placed as a Non-Finalist at Miss World. Julie Lee-Ann Martin was.\"\n\n",
        "expected_content": "",
        "optional_list": []
    },
    {
       "task_name": "squad",
        "task_instruction": "Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.",
        "examples": "\n[input]=\"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\n[output]=\"Santa Clara\"\n\n[input]=\"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [var\u02c8\u0282ava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\"\n[output]=\"Vistula River\"\n\n[input]=\"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\"\n[output]=\"Europe\"\n",
        "expected_content": "Question and Context",
        "optional_list": [
            "input",
            "output"
        ],
        "metric": "exact_match"
    }
]